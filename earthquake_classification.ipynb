{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "earthquake_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdcfipc6KkWm"
      },
      "source": [
        "# Import Required Libraries\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ryMo6oZKqVp"
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "import plotly.graph_objects as go\n",
        "from itertools import cycle\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from scipy import interp\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn import tree\n",
        "from sklearn import preprocessing\n",
        "from sklearn.inspection import permutation_importance\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJWU3TjgKLJD"
      },
      "source": [
        "# Data Cleaning\n",
        "---\n",
        "Following columns are ignored\n",
        "\n",
        "* Sl. No., Reference are just description columns\n",
        "* Location is redundant as latitude and longitude are already given\n",
        "* ORIGIN TIME - UTC and IST have very less number values. Hence they are ignored as they don't add much value either\n",
        "* Mb, Ms, ML are derivatives of Mw. Hence ignored\n",
        "* Intensity, MMI, MME are also dropped as they have very less number of values\n",
        "\n",
        "Column Mw is the parameter that we have to classify and Lat, Lon are important parameters that should not be guessed. Hence removing rows with null Mw, Lat, Lon values.\n",
        "\n",
        "* Year, Date are filled with stratergy 'ffill'.\n",
        "* Depth is filled with mean of data grouped by Mw. Few still remaining values are filled with mean.\n",
        "* Mw column has about 50k values, while MAGNITUDE has only 40k. So used Mw is used renamed the column as MAGNITUDE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbYSvrr0IyF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a00d75-1ef1-4b84-c05a-25d53bcc68e0"
      },
      "source": [
        "# Followed above explanation and converted all to appropriate data types\n",
        "data_path = 'https://raw.githubusercontent.com/nee2shaji/DataAnalytics/master/IndianEarthquakes.csv'\n",
        "eq_data = pd.read_csv(data_path, encoding = 'unicode_escape', skiprows=10, names=['Sl.No.','YEAR' ,'MONTH' ,'DATE','ORIGIN TIME(UTC)','ORIGIN TIME(IST)','MAGNITUDE','Mw','Mb1','Mb2','Ms','ML','LAT','LONG','DEPTH','INTENSITY','MMI','MME','LOCATION','REFERENCE'])\n",
        "\n",
        "eq_data = eq_data[['YEAR' ,'MONTH' ,'DATE', 'Mw','LAT','LONG','DEPTH']]\n",
        "eq_data.columns = ['YEAR' ,'MONTH' ,'DATE', 'MAGNITUDE','LAT','LONG','DEPTH']\n",
        "print(eq_data.count(axis=0))\n",
        "\n",
        "# Magnitude Lat Long to float\n",
        "eq_data['MAGNITUDE'] = eq_data['MAGNITUDE'].astype(str).str.extract('([0-9]+\\.?[0-9]*)').astype(float)\n",
        "eq_data = eq_data[~eq_data['MAGNITUDE'].isna()]\n",
        "\n",
        "# Converting Lat Long to float\n",
        "eq_data['LAT'] = eq_data['LAT'].astype(str).str.extract('([0-9]+\\.?[0-9]*)').astype(float)\n",
        "eq_data['LONG'] = eq_data['LONG'].astype(str).str.extract('([0-9]+\\.?[0-9]*)').astype(float)\n",
        "eq_data = eq_data[~eq_data['LAT'].isna()]\n",
        "\n",
        "#Year, Date are filled with stratergy 'ffill'.\n",
        "eq_data['YEAR'] = pd.to_numeric(eq_data['YEAR'], errors=\"coerce\").fillna(method='ffill').astype(int)\n",
        "eq_data = eq_data[eq_data['YEAR']>1800]\n",
        "eq_data['MONTH'] = pd.to_numeric(eq_data['MONTH'], errors=\"coerce\").fillna(method='ffill').astype(int)\n",
        "eq_data['DATE'] = pd.to_numeric(eq_data['DATE'], errors=\"coerce\").fillna(method='ffill').astype(int)\n",
        "\n",
        "#Depth is filled with mean of data grouped by Mw\n",
        "eq_data['DEPTH'] = eq_data['DEPTH'].astype(str).str.extract('([0-9]+\\.?[0-9]*)').astype(float)\n",
        "eq_data['DEPTH'].fillna(eq_data.groupby(['MAGNITUDE'])['DEPTH'].transform('mean'), inplace=True)\n",
        "eq_data['DEPTH'].fillna(eq_data['DEPTH'].mean(), inplace=True)\n",
        "\n",
        "print(\"\\n\\nChecking if there are any more NaN values\")\n",
        "print(eq_data.isna().sum())\n",
        "print(\"\\n\\nPrinting count of all columns\")\n",
        "print(eq_data.count(axis=0))\n",
        "\n",
        "final_data = eq_data[['YEAR' ,'MONTH' ,'DATE', 'MAGNITUDE','LAT','LONG','DEPTH']]\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning:\n",
            "\n",
            "Columns (2,6,7,9,10,11,12,13,15,16,17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "YEAR         52989\n",
            "MONTH        52971\n",
            "DATE         52932\n",
            "MAGNITUDE    50486\n",
            "LAT          52989\n",
            "LONG         52989\n",
            "DEPTH        50811\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "Checking if there are any more NaN values\n",
            "YEAR         0\n",
            "MONTH        0\n",
            "DATE         0\n",
            "MAGNITUDE    0\n",
            "LAT          0\n",
            "LONG         0\n",
            "DEPTH        0\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "Printing count of all columns\n",
            "YEAR         50424\n",
            "MONTH        50424\n",
            "DATE         50424\n",
            "MAGNITUDE    50424\n",
            "LAT          50424\n",
            "LONG         50424\n",
            "DEPTH        50424\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_eRFG9Q_bPh"
      },
      "source": [
        "# Selecting Threshold\n",
        "---\n",
        "\n",
        "* This step we try to find out the threshold. The value of the threshold whether or not to call it an earthquake is given by the value of Mw that they have asked us to choose from [4,5].\n",
        "* Since better classification can be done when the classes are more balanced. We suggest we choose the cut off in such a way that the number of samples in each class are almost the same.\n",
        "* In the next code block we are trying to do the above step and also provide the histogram to see the distribution of counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5KWZHlJTyEI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "b80e8173-db9d-41df-e1c4-b903bc996b78"
      },
      "source": [
        "print(\"ThresholdSetting\", \"Class1Count\", \"Class2Count\")\n",
        "for T in np.linspace(4,5,5):\n",
        "  print(\"{:.2f}\".format(T), eq_data[eq_data['MAGNITUDE']>=T]['MAGNITUDE'].count(),eq_data[eq_data['MAGNITUDE']<T]['MAGNITUDE'].count())\n",
        "  \n",
        "plt.figure()\n",
        "plt.hist(eq_data['MAGNITUDE'], 100, range=(2.5, 8), density=True, facecolor='b', alpha=0.75)\n",
        "plt.xlabel('Magnitude (Mw)')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(True)\n",
        "\n",
        "# Setting threshold as 4.5 as it has the most balanced dataset\n",
        "T = 4.5\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ThresholdSetting Class1Count Class2Count\n",
            "4.00 45980 4444\n",
            "4.25 35855 14569\n",
            "4.50 25757 24667\n",
            "4.75 16229 34195\n",
            "5.00 9503 40921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qkws1cZAM2K"
      },
      "source": [
        "# Train Test Splits\n",
        "---\n",
        "We have decided the threshold to be 4.5. The next step is to define y (the output) based on values of Mw and the Train Test Splits, we have chosen ```[Train, Test] Ratio = [0.8, 0.2]``` and used ```StandardScaler()``` as a preprocessing step before feeding into the KNN classifier. For Decision Tree direct input is taken in its raw form without any preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfynTWV6dP3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ab7e0b-2e48-4667-f943-283719a1a4cf"
      },
      "source": [
        "X = eq_data[['YEAR' ,'MONTH' ,'DATE','LAT','LONG','DEPTH']]\n",
        "y = eq_data['MAGNITUDE'] >= T\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=333)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, X_test.shape)\n",
        "\n",
        "plt.style.use('seaborn')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40339, 6) (10085, 6) (40339,) (10085, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-56tTTa3BK_B"
      },
      "source": [
        "# Part 1 : ROC Curve for KNN \n",
        "---\n",
        "We have used K  (number of neighbours) as parameter in KNN and tuned this value of K to get  a number of classifiers with their respective ROC Curves, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faUKGeM6iIlX"
      },
      "source": [
        "# KNN Classifier\n",
        "\n",
        "neighbors = np.arange(5, 20)\n",
        "#neighbours = np.linspace(5,51,num = 15)\n",
        "knn_scores = pd.DataFrame(columns=[\"Neighbors\", \"Training Set Score\", \"Test Set Score\", \"AUC Score\"])\n",
        "tr_scr = []\n",
        "tes_scr = []\n",
        "auc_scr=[]\n",
        "\n",
        "#fig = go.Figure()\n",
        "for i, k in enumerate(neighbors):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    # ROC curve\n",
        "    knn_pred_prob = knn.predict_proba(X_test_scaled)\n",
        "    knn_fpr, knn_tpr, knn_thresh = roc_curve(y_test, knn_pred_prob[:,1])\n",
        "    knn_scores.loc[i, \"Neighbors\"] = k\n",
        "    auc_scr.append(roc_auc_score(y_test, knn_pred_prob[:,1]))\n",
        "    knn_scores.loc[i, \"AUC Score\"] = roc_auc_score(y_test, knn_pred_prob[:,1])\n",
        "    tr_scr.append(knn.score(X_train_scaled, y_train))\n",
        "    knn_scores.loc[i, \"Training Set Score\"] = knn.score(X_train_scaled, y_train)\n",
        "    tes_scr.append(knn.score(X_test_scaled, y_test))\n",
        "    knn_scores.loc[i, \"Test Set Score\"] = knn.score(X_test_scaled, y_test)\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    plt.plot(knn_fpr, knn_tpr, linestyle='-', label=k)\n",
        "    #fig.add_trace(go.Scatter(x=knn_fpr, y=knn_tpr, name=str(k)))\n",
        "\n",
        "plt.title('ROC curve for KNN with varying k')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best', title=\"K Neighbours\")\n",
        "plt.show() \n",
        "\n",
        "#fig.show()\n",
        "knn_scores\n",
        "# The curve will look better when we reduce the number of neighbors plotted. Right now plotted many to know the training and test scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCy_7jo32P3_"
      },
      "source": [
        "\n",
        "fig = go.Figure(data=go.Scatter(x=neighbors, y=auc_scr,name = 'AUC'))\n",
        "fig.add_trace(go.Scatter(x=neighbors, y=tr_scr,name = 'Train Score'))\n",
        "fig.add_trace(go.Scatter(x=neighbors, y=tes_scr,name = 'Test Score'))\n",
        "fig.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YouLgaWOQDou"
      },
      "source": [
        "# Part 1 : ROC Curve for Decision Tree\n",
        "---\n",
        "We have used Pre-prune depth as parameter in Decision Tree and tuned this value of depth to get  a number of classifiers with their respective ROC Curves, as shown below. Procedure is similar to KNN classifier, the best classifer has the maximum AUC. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h05MeKs1jFX8"
      },
      "source": [
        "# Decision Tree Classifier\n",
        "\n",
        "depths = np.arange(5, 20)\n",
        "dt_scores = pd.DataFrame(columns=[\"Depth\", \"Training Set Score\", \"Test Set Score\", \"AUC Score\"])\n",
        "\n",
        "for i, d in enumerate(depths):\n",
        "    clf = DecisionTreeClassifier(max_depth=d)\n",
        "    clf.fit(X_train, y_train)\n",
        "    # ROC curve\n",
        "    clf_pred_prob = clf.predict_proba(X_test)\n",
        "    clf_fpr, clf_tpr, clf_thresh = roc_curve(y_test, clf_pred_prob[:,1])\n",
        "    dt_scores.loc[i, \"Depth\"] = d\n",
        "    dt_scores.loc[i, \"AUC Score\"] = roc_auc_score(y_test, clf_pred_prob[:,1])\n",
        "    dt_scores.loc[i, \"Training Set Score\"] = clf.score(X_train, y_train)\n",
        "    dt_scores.loc[i, \"Test Set Score\"] = clf.score(X_test, y_test)\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    plt.plot(clf_fpr, clf_tpr, linestyle='-', label=d)\n",
        "\n",
        "plt.title('ROC curve for Decision Tree with varying depth')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best', title=\"Max Depth\")\n",
        "plt.show() \n",
        "\n",
        "dt_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z3-2d8GQh4N"
      },
      "source": [
        "As shown in the table above the AUC curve value peaks at Desicion Tree with a depth of 9 and falls steadly after that. So therefore the best parameter for Decision tree is ```max_depth = 9``` ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkodYcLsRE6P"
      },
      "source": [
        "# Part 2 - Better Classifier Among the two ?\n",
        "\n",
        "Decision Tree seems to be the better classifier based on the AUC and also the Train, Test Accuracy Scores. Another observation from the observations below is that there is a trade off between the Training Set Score and the AUC. The best AUC doesn't guarantee a Training Set Score but improves the performance of the Test Set Score in both the classifiers.\n",
        "\n",
        "The main deciding factors on which classifier is better is the Test Set Score and the AUC which is better in case of Decision Tree than KNN classifier.\n",
        "\n",
        "```\n",
        "KNN\n",
        "Neighbours\tTraining Set Score\tTest Set Score\tAUC Score\n",
        "5\t          0.771363\t          0.665444\t       0.716523\n",
        "18\t         0.7168\t            0.680714\t       0.74725 (best)\n",
        "\n",
        "Decision Tree\n",
        "Depth\tTraining Set Score\tTest Set Score\tAUC Score\n",
        "9\t     0.714247\t           0.700347\t      0.76697 (best)\n",
        "19\t    0.902006\t           0.657908\t      0.667574\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKIa4koiS7Z_"
      },
      "source": [
        "# Part 3 -  Best possible values of the parameters\n",
        "---\n",
        "The values of the parameters of the  classifiers are decided based on the ROC. \n",
        "The ROC curve shows the trade-off between ```True Positive Rate``` and ```(1 – False Positive Rate)```. Classifiers that give curves closer to the top-left corner indicate a better performance. As a baseline, a random classifier is expected to give points lying along the diagonal (FPR = TPR). The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.The curve which attains the maximum area under the curve (AUC) is said to be the better model. \n",
        "\n",
        "![1_pk05QGzoWhCgRiiFbz-oKQ.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAWkAAAFKCAIAAABQITmBAAAMZklEQVR42uzdP2sbaR7A8dGSd2ECyyKlOFzdNUaxr9hOPjBpcoRttpPZJlKjLmU6N3KaxerSLOHSHIazui3WRKS6SqSIzHIQ8jrmsGb0fyxnFEkZzXw+bOGVHhtnsvny/OaZrB+EYRgApPSdSwBoB6AdgHYA2gGgHYB2ANoBaAegHQDaAWgHoB2AdqRWGvHbA9v3t+vvvuRP3wNXCgpeirlX/vv38K9/aAdwXyxWCId2QKFLkRiO89If2gFicb8oFkdHR9fX11/+WdoBxYpF4rYibTi0A4qyrVhihXBoBxRiW7H2cGgHFC4Wc1YLh3ZADmeQ7dAOyPO2Yr1zinZAsWKx9nBoB+S8FBsKh3ZAzmOxoXBoB+Q5FpsLh3bAmmORkVJsOhzaAfmMxabDoR2Qw1hsIRzagVjkLRbbCYd2IBa5isXWwqEdiEV+YrHNcGgHepFDWwiHdiAWwqEd6EUhYzFnO+HQDsQC7UAvimprc4p2sMO9EIsshEM70Avh0A4MI8KxRX4O/upuzh+XFj0+v0la3D39snULK+9ct2vJiP5ZHovpf/wHluVwBEFQCsMwaxdl/DO4M/i9zbWj0uwlvVNtD941ylMxOO4kf4n6VXhRm8lG0srZL5erLYZGZDYch4eH9h2bVm0PwsigXR2+0muedSeBiXMwWRaGV/Xo3c7xaTdYWHmblHDuS+7a/mJJOGwudn3H4X7H+pUbL+rNYQD6H2+CWjkIumfxzqR+Nb11qF0M2v3hpqXz8rxVa8yunN6LlBvvwob9BRkLh3ZszP6jYSi6/x7tOVq1ucz845/VZq8XBL0PgyAoj1cG9Se1nIwkepHjcGjHmnVP46kjDsDNx/5MSmbi8Wg/CHqjLUowWpn5dEiGcGjHuvSalVJz9v7HRS1Xv0K9EA7t2LgdPRNZIRl6UeRwOGdZVy4mJyK9ZmXyRMZwLgniuWTO7DyzbOU3SMaSgxJHJMKhHWtWbryOzlOnTmgrf4kPbf/1n7kkTI5Vohsc45WTT85OMpyqCod2bLgeL6LnNjov463H+JVeszJ5kmPmoY/4AGa8MugczzxK2j0tlU673zYZfmu/lcyGQzvWrfakPrd7qF1MngMbP2deSXroo3YxebSsMnkm/a5HUiUj39uN7H+T2rHmeLTiP//jrUdQu0h6OnT46OjccUy58W7yyOn03ZT1HdtIhjllXfx9lqJYcu/TxRGORff+fRZntJKBcKxCO4qVDNUQDu3ARkM4tIONVUMyhEM7sNEQDu1ANdjlcGiH8QTh0A7VQDi0A9UQDu1ANchbOLRDNRAO7VANMi834dAO1YAV+Tv4WQ+Hvxefgzkll78u+45MV8NlyUE48jSnaIdqIBxmFuFAOOw7VMNlEQ7tIEU4VEM4tAPbDeG4LsKvVDtsNxCOVbhXKhwIh32HaiAc9h3CgXBoh3AIh3CYWVgpHKohHNqB7QaxwobDzCIcoB3CwbbmFBdBO4SD1OEo8pwyzf2OjYRDNYTDvgPhQDi0QzgQDu0QDoRDO4QD4dAO4UA4tAPhEA60QzgQDu0A4dAOmw6EQzuEA+HQDoRDONAO4WA54dCOzRIO0I5VNh3kbE5xEbTDtELqcJhTtMO0gnBoRyY3HcIhHGjHKtMKwoF2pA6HTYdwoB2pCYdwoB2mFeFAO0wrCId2mFYQDu0wrSAc2mHTgXCgHTYdwoF22HRwD+HQjm1vOoQDtMO0Urg5xUXQDtMKqcNhTtEOmw6EQztsOhAO7bDpQDjQDpsO4UA7EA60Y/sDi02HcKAdCAfaYdOBcGgHCId22HQgHNoBwoF2pNp0IBxoxyoMLLtCOLQD0I4dH1hsOrI/p7gI2gGpw2FO0Y4sbjoQDrRjFQYW4UA7EA60YysDi02HcKAdCAfagXCgHQYWhEM7QDjQDoQD7TCwCAfagXCgHbCUcGiHgQW0AzY8p7gI2gGpw2FOKUo7TktLPD6/CW7OHy++cdqdfIXEBcNPNbAIBwXdd+w/KgeDD73FNzrHs/1Y1GtWttwPhINttuMiHLmqR6/Ur8YvXdQmC8cvj9Z1Xs6modoexCsG7Wrcj7Ouqy8cuN8RqbVGafgwSF5RbryI+9L/eLPFgQXhIMvtSDv0bI+bHcJBhttxc/5zM7oDUn9SS17SPT3uLF+BcLALHqzji3SOS53pf6+2W7Nh6DUrpebMgsGFdAgHhW/HtPpVeE8Wqu3Bu8Y25hWns8JBxmeWqeOX5HDE5yzRIUuvWXFCKxxoRwrlxuvoHMYJrXCgHSnrER/Rzj8BYmARDrRjqdqTKB62HsKBdqSLx+j5sY1vPdga4SigUhiGmfueSqXog6/53sws8DUODw+ztu/YBuHYzpziIphZIHU4zCnaAcKBdiAcaMdq3OwQDrQD4UA7EA60A+FAO3LFzQ7hQDsQDrQD4UA7EA7IVTv8RAXhQDu+ihulX0k4MLMA2sGG5xQXAe0gdTjMKRS0HZ4KEw60A+FAOxAOtAPhAO0QDuGg8O1wo1Q40A6EA+1AONAOhANy1Q5/fVY40I6v4kapcKAdCAfagXCgHewW4UA7AO1YH0+UJs4pLgLaQepwmFPQDoQD7UA40A6EA+3YYW6UCgfagXCgHQgH2oFwgHYIBxS+HUX+33YIB9qxBkU7ZBEOtINVCAfaAWgHG55TXAS0g9ThMKegHetRnKfRhQPtQDjQDoQD7UA4IK0HLoFwZMrnt788e9Uffrj//M2vT/eSl70/O2pdTr9w5+K5lcu+KPYdwrG76ej93h993P+99zk5Bkdz4bhd/OrZ0dHZ+3tX9l89++XtZ/+ZFbQdOT5kKfqoEqfj5OQk+nP+2/vFbUkcg/3nb65Hzk6idy9bk3pMVgYnZ+OVb57v+1Nv32HHkdt0HLWOohpcXs/tJH6L55mTs+nJ46A1asLl63hTMbXyunUwXrn39NdrM4t2CEde03EQHCTG4/31aM/x08Hsp+5Vf4zi0f/fp+mVwy+GdghHrsVbhf3vH95uJRLi8fnTn9EHPzxc2DnsPfwh+uDPT5+nVkqHdghH/tMRbRX2f6wOw3Bwx9iCdiAcS9IhHlm3e8935OyQRTjmbmX0Xz07ejX71uXrtz8dPN2L55J+PJcczI4tc/PMkpXYd5DDdCQaP+jx8Pv92Rcmnz8+VolucIxXLh7zoh27PKe4CMkDy9QzG9MPboxasff055PR7mTqObDphz7iA5jxyuCyNfMs2PuzhUfIKMjM4gZH/nx++3ruXsfIwdFJcHkZxePp7dhy0Do7uRyG4rJ1NL9VmXno46D15vmfw8fbF8agE5fcvkM4ciF+Dn0xHZMbppMp5aCV9HTo8NHR1ux57N7TXyePnI7cbm1azm3XoBSGYea+p1Ip+iDxe9vde6XCwQ45PDzM1b5DOMDMYlQB7UA40A6EA7RDOEA7hAO0QzhAO7ZkVw5ohQPtQDhAO4QDtCObhAPtANCODc8pLgLakV3ZPGRxgwPtQDhAO4QDtEM4QDuEA7RDOEA7EA7IYTuycEArHGDfIRygHcIB2iEcoB3CAdpRVMIBO9yO3f1ZcKAdxZpTXATQjtThMKeAdggHaIdwgHYIB2iHcIB2ZM+mD2iFA+w7hAO0QzhAO4QDtEM4gEK2QzhAO4QDtCPJeg9ohQPsO1YhHKAdgHZskv8ZB2jHKuEwp4B2CAdoh3CAdmxa2gNa4QDtsOMA7RAO0A7hAO0QDqCQ7RAO0I559x6yCAdohx0HaIdwgHZkk3CAdgDaseE5xe8ZaEfqcJhTQDvuN31AKxygHXYcoB3CAYVUCsMwc99TqeQ3Br6te8vgjBbI6cwCZNCDXdwsAfYdgHYA2gGgHYB2ANoBaAegHQDaAWgHa9I9LSU47Q7fvDl/fOd7dy8oPT6/cWG1g3y7+dhf8u7gQ2/xxc7xbD8W9ZoV/dAOiqHaHoRTLmoz79av4tev6nE/Xs6mYfLpg3Y17sdZ12XVDojUWqM0fBgkryg3XsR96X+089AOSG//UdlF0A4IhrdGf25Gd0DqT2rJS7qnx53lK9AOcqTXrNxxlBLEt0dvVeJyVNut2h2fHpWj2h5cSEcxPHAJ+DL1q/CeLFTbg3cN84p9B4Uwe84yH4fxOUvCe9OfHh2y9JoVJ7TaASmUG6+jcxgntNoBKesRH9HOPwGCdsBStSdRPGw9tAPSxWP0/JitRyGU/EADwL4D0A5AOwDtANAOQDsA7QC0A9AOAO0AtAPYov8HAAD//0/USFVHooMhAAAAAElFTkSuQmCC)\n",
        "\n",
        "\n",
        "\n",
        "## So in our case, KNN shows the maximum AUC with ```k=18``` and Desicion Tree shows the best AUC with ```depth = 9```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq6Re-UARBTQ"
      },
      "source": [
        "# Part 4 - Subset of two features\n",
        "---\n",
        "In this section we are trying to find the two most important features that we can use to determine the same classification problem. There are many ways to do this. We follow two different methods for KNN and Decision Tree.\n",
        "\n",
        "In KNN, we use ```permutation_importance``` to find the important features based on the decrease in a model score when a single feature value is randomly shuffled. Out of these attributes we choose the top two features with the most model score.\n",
        "\n",
        "In the Decision Tree, we have used the inbuilt function ```feature_importances_``` to find the most important features as shown in the graph below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmMct3ohB_j-"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=18)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "results = permutation_importance(knn, X_test_scaled, y_test, scoring='accuracy')\n",
        "importance = results.importances_mean\n",
        "for i,v in enumerate(importance):\n",
        "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
        "# plot feature importance\n",
        "plt.bar(['YEAR', 'MONTH', 'DATE', 'LAT', 'LON', 'DEPTH'], importance)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00lvPwqjbfLy"
      },
      "source": [
        "To select the most two important features for KNN Nearest Neighbour, we look at the model scores of the columns when shuffled. As observed above the features with the top scores are **YEAR** followed by **LATITUDE**.\n",
        "\n",
        "The code below uses just these two features as inputs to the KNN classifer to determine the same output as we did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNETFZRDVLw2"
      },
      "source": [
        "# KNN Classifier with 2 most important features \n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_trunc = X_train[['YEAR' ,'LAT']]\n",
        "scaler.fit(X_trunc)\n",
        "X_train_trunc = scaler.transform(X_trunc)\n",
        "X_trunc = X_test[['YEAR' ,'LAT']]\n",
        "X_test_trunc = scaler.transform(X_trunc)\n",
        "#neighbors = np.arange(5, 50)\n",
        "knn_scores = pd.DataFrame(columns=[\"Neighbors\", \"Training Set Score\", \"Test Set Score\", \"AUC Score\"])\n",
        "k = 18\n",
        "i = 0\n",
        "#for i, k in enumerate(neighbors):\n",
        "knn = KNeighborsClassifier(n_neighbors=k)\n",
        "knn.fit(X_train_trunc, y_train)\n",
        "# ROC curve\n",
        "knn_pred_prob = knn.predict_proba(X_test_trunc)\n",
        "knn_fpr, knn_tpr, knn_thresh = roc_curve(y_test, knn_pred_prob[:,1])\n",
        "knn_scores.loc[i, \"Neighbors\"] = k\n",
        "knn_scores.loc[i, \"AUC Score\"] = roc_auc_score(y_test, knn_pred_prob[:,1])\n",
        "knn_scores.loc[i, \"Training Set Score\"] = knn.score(X_train_trunc, y_train)\n",
        "knn_scores.loc[i, \"Test Set Score\"] = knn.score(X_test_trunc, y_test)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.plot(knn_fpr, knn_tpr, linestyle='-', label=k)\n",
        "\n",
        "plt.title('ROC curve for KNN with k = 18 and top two features')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best', title=\"K Neighbours\")\n",
        "plt.show() \n",
        "\n",
        "knn_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywp6Rd6KH15S"
      },
      "source": [
        "# Feature Importance of Decision Tree\n",
        "\n",
        "col_sorted_by_importance=clf.feature_importances_.argsort()\n",
        "\n",
        "dt_feat_imp = pd.DataFrame({\n",
        "    'cols' : X.columns[col_sorted_by_importance],\n",
        "    'imps' : clf.feature_importances_[col_sorted_by_importance]\n",
        "})\n",
        "\n",
        "dt_feat_imp.plot(kind = \"barh\", x='cols', y='imps')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW11fXYzVXOu"
      },
      "source": [
        "The top two features are **Latitude** followed by **Longitude** when Threshold is 4.5 as descirbed in the above plot. The below plot gives us feature pairwise plot of decision tree of additional insights between the top two features.\n",
        "\n",
        "This cell is followed by the code for a Decision Tree classifer with only use of the top two features deduced by the ```feature_importances_``` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nceMWXrmJPtb"
      },
      "source": [
        "clf = DecisionTreeClassifier(max_depth=7)\n",
        "X_plot = X_train[['LAT', 'LONG']]\n",
        "clf.fit(X_plot, y_train)\n",
        "#tree.plot_tree(clf);\n",
        "x_min, x_max = X_plot.loc[:, 'LAT'].min() - 1, X_plot.loc[:, 'LAT'].max() + 1\n",
        "y_min, y_max = X_train.loc[:, 'LONG'].min() - 1, X_plot.loc[:, 'LONG'].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 1), np.arange(y_min, y_max, 1))\n",
        "plt.figure()\n",
        "plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
        "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
        "plt.xlabel('Latitude')\n",
        "plt.ylabel('Longitude')\n",
        "plt.show()\n",
        "# Reference : https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py\n",
        "# This is a feature pairwise plot of decision tree. I just chose two most important pair of features (Latitude and Year) and plotted them.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NqKHAxwV_j9"
      },
      "source": [
        "# Decision Tree Classifier with depth = 9 and top two features \n",
        " \n",
        "X_train_trunc = X_train[['LONG' ,'LAT']]\n",
        "X_test_trunc = X_test[['LONG' ,'LAT']]\n",
        "\n",
        "#depths = np.arange(5, 20)\n",
        "dt_scores = pd.DataFrame(columns=[\"Depth\", \"Training Set Score\", \"Test Set Score\", \"AUC Score\"])\n",
        "\n",
        "#for i, d in enumerate(depths):\n",
        "d = 9 \n",
        "clf = DecisionTreeClassifier(max_depth=d)\n",
        "clf.fit(X_train_trunc, y_train)\n",
        "# ROC curve\n",
        "clf_pred_prob = clf.predict_proba(X_test_trunc)\n",
        "clf_fpr, clf_tpr, clf_thresh = roc_curve(y_test, clf_pred_prob[:,1])\n",
        "dt_scores.loc[i, \"Depth\"] = d\n",
        "dt_scores.loc[i, \"AUC Score\"] = roc_auc_score(y_test, clf_pred_prob[:,1])\n",
        "dt_scores.loc[i, \"Training Set Score\"] = clf.score(X_train_trunc, y_train)\n",
        "dt_scores.loc[i, \"Test Set Score\"] = clf.score(X_test_trunc, y_test)\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.plot(clf_fpr, clf_tpr, linestyle='-', label=d)\n",
        "\n",
        "plt.title('ROC curve for Desicion Tree with depth =9 and top 2 features')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best', title=\"Max Depth\")\n",
        "plt.show() \n",
        "\n",
        "dt_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76rlUAPm03WR"
      },
      "source": [
        "# Part 5 : Feature Processing\n",
        "---\n",
        "We have used inbuilt ```PolynomialFeatures``` for additional feature processing.  In this we generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to a given degree mentioned by us. We have taken the default value of degree as 2 itself and we have mentioned ```interaction_only = True``` to take only the features that are products of at most degree distinct input features.\n",
        "\n",
        "We have used the Desicion Tree with the best configuration after the feature processing of parameters and compared it with the existing Decision Tree (without feature processing).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZlBJT7S7L17"
      },
      "source": [
        "# Q5 Additional features\n",
        "poly = PolynomialFeatures(interaction_only=True)\n",
        "#poly = PolynomialFeatures(6)\n",
        "depths = np.arange(5, 11)\n",
        "dt_scores = pd.DataFrame(columns=[\"Depth\", \"Training Set Score\", \"Test Set Score\", \"AUC Score\"])\n",
        "\n",
        "for i, d in enumerate(depths):\n",
        "    clf = DecisionTreeClassifier(max_depth=d)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    clf.fit(X_train_poly, y_train)\n",
        "    X_test_poly = poly.fit_transform (X_test)\n",
        "    # ROC curve\n",
        "    clf_pred_prob = clf.predict_proba(X_test_poly)\n",
        "    clf_fpr, clf_tpr, clf_thresh = roc_curve(y_test, clf_pred_prob[:,1])\n",
        "    dt_scores.loc[i, \"Depth\"] = d\n",
        "    dt_scores.loc[i, \"AUC Score\"] = roc_auc_score(y_test, clf_pred_prob[:,1])\n",
        "    dt_scores.loc[i, \"Training Set Score\"] = clf.score(X_train_poly, y_train)\n",
        "    dt_scores.loc[i, \"Test Set Score\"] = clf.score(X_test_poly, y_test)\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    plt.plot(clf_fpr, clf_tpr, linestyle='-', label=d)\n",
        "\n",
        "plt.title('ROC curve for Desicion Tree with varying depth')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive rate')\n",
        "plt.legend(loc='best', title=\"Max Depth\")\n",
        "plt.show() \n",
        "\n",
        "dt_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czhq9WSzqDHh"
      },
      "source": [
        "Taking the max AUC, the depth of the tree is determined to be 8. This is the best configuration of the Decision Tree after feature processing.\n",
        "\n",
        "\n",
        "The accuracy both train and test has been improved by a small bit when compared to the previous methods without feature processing, as mentioned below.\n",
        "\n",
        "\n",
        "```\n",
        "Shape of Input : (40339, 6)\n",
        "Before feature processing\n",
        "\tDepth\tTraining Set Score\tTest Set Score\tAUC Score\n",
        "0\t5\t    0.848286\t          0.85642\t       0.765079\n",
        "1\t6\t    0.849748\t          0.85642\t       0.774639\n",
        "2\t7\t    0.853814\t          0.856718\t      0.780776\n",
        "3\t8\t    0.857929\t          0.85642\t       0.781087\n",
        "4\t9\t    0.861524\t          0.858304\t      0.782403\n",
        "5\t10\t   0.86668\t           0.853743\t      0.783854\n",
        "\n",
        "Shape of Input : (40339, 22)\n",
        "After feature processing\n",
        "\tDepth\tTraining Set Score\tTest Set Score\tAUC Score\n",
        "0\t5\t    0.848335\t          0.856222\t      0.763528\n",
        "1\t6\t    0.850343\t          0.854636\t      0.771735\n",
        "2\t7\t    0.856838\t          0.859296\t      0.776799\n",
        "3\t8\t    0.859243\t          0.858899\t      0.777935\n",
        "4\t9\t    0.863383\t          0.854933\t      0.780008\n",
        "5\t10\t   0.869779\t          0.854636\t      0.779554\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqVuLXjL2DO_"
      },
      "source": [
        "# Additional Analysis based on Threshold\n",
        "---\n",
        "In this code block we have looked at the range of thresholds and how it affects the performance. For reference the class balance is also shown in the code also. One intriguing observation is that when the class is not properly balanced or skewed the performance is better than the balanced class case. One case using KNN is shown where the threshold is 4.0 in first case and 4.5 in second. The rest of the thresholds and their results can be observed in the code block below.\n",
        "```\n",
        "------------------------------------------------------------------------------\n",
        "Threshold=4.0, [Class1, Class2] Counts = [45980, 4444]\n",
        "  Neighbors Training Set Score Test Set Score Precision    Recall  F1_score\n",
        "0         5           0.931158        0.91056  0.931683  0.973384  0.952077\n",
        "1         6            0.92744       0.906991  0.936899  0.962955  0.949748\n",
        "2         7           0.927093       0.913634  0.929676  0.979468  0.953923\n",
        "3         8           0.924986       0.911056  0.933793  0.971429  0.952239\n",
        "4         9           0.924267       0.914725  0.929756  0.980663  0.954531\n",
        "------------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------------\n",
        "Threshold=4.5, [Class1, Class2] Counts = [25757, 24667]\n",
        "  Neighbors Training Set Score Test Set Score Precision    Recall  F1_score\n",
        "0         5           0.771363       0.665444  0.680944  0.660512  0.670572\n",
        "1         6           0.751927       0.662568  0.719882  0.565493  0.633416\n",
        "2         7            0.74873       0.668121  0.685348  0.658588  0.671702\n",
        "3         8           0.738219       0.668418  0.719631  0.584535  0.645086\n",
        "4         9           0.739632       0.671096  0.689718  0.658011  0.673491\n",
        "------------------------------------------------------------------------------\n",
        "```\n",
        "Regardless we have taken the balanced class case itself for all our experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KEaw8zxcH0V"
      },
      "source": [
        "\n",
        "cnt = [[45980, 4444],[35855, 14569],[25757, 24667],[16229, 34195],[9503, 40921]]\n",
        "j = 0\n",
        "\n",
        "for T in np.linspace(4,5,5):\n",
        "  X = eq_data[['YEAR' ,'MONTH' ,'DATE','LAT','LONG','DEPTH']]\n",
        "  y = eq_data['MAGNITUDE'] >= T\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=333)\n",
        "\n",
        "  print(\"------------------------------------------------------------------------------\")\n",
        "\n",
        "  print(\"Threshold=\"+str(T)+', [Class1, Class2] Counts = '+str(cnt[j]))\n",
        "  j+=1\n",
        "  neighbors = np.arange(5, 10)\n",
        "\n",
        "  knn_scores = pd.DataFrame(columns=[\"Neighbors\", \"Training Set Score\", \"Test Set Score\", \"Precision\", \"Recall\", \"F1_score\"])\n",
        "\n",
        "  for i, k in enumerate(neighbors):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    # ROC curve\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    knn_scores.loc[i, \"Neighbors\"] = k\n",
        "    knn_scores.loc[i, \"Training Set Score\"] = knn.score(X_train_scaled, y_train)\n",
        "    knn_scores.loc[i, \"Test Set Score\"] = knn.score(X_test_scaled, y_test)\n",
        "    knn_scores.loc[i, \"Precision\"] = precision_score(y_test, y_pred)\n",
        "    knn_scores.loc[i, \"Recall\"] = recall_score(y_test, y_pred)\n",
        "    knn_scores.loc[i, \"F1_score\"] = f1_score(y_test, y_pred)\n",
        "  print('Using KNN')\n",
        "  print(knn_scores)\n",
        "  print(\"------------------------------------------------------------------------------\")\n",
        "  depths = np.arange(5, 10)\n",
        "  dt_scores = pd.DataFrame(columns=[\"Depth\", \"Training Set Score\", \"Test Set Score\", \"Precision\", \"Recall\", \"F1_score\"])\n",
        "\n",
        "  for i, d in enumerate(depths):\n",
        "    clf = DecisionTreeClassifier(max_depth=d)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    dt_scores.loc[i, \"Depth\"] = d\n",
        "    dt_scores.loc[i, \"Training Set Score\"] = clf.score(X_train, y_train)\n",
        "    dt_scores.loc[i, \"Test Set Score\"] = clf.score(X_test, y_test)\n",
        "    dt_scores.loc[i, \"Precision\"] = precision_score(y_test, y_pred)\n",
        "    dt_scores.loc[i, \"Recall\"] = recall_score(y_test, y_pred)\n",
        "    dt_scores.loc[i, \"F1_score\"] = f1_score(y_test, y_pred)\n",
        "  print('Using Decision Tree')\n",
        "  print(dt_scores)\n",
        "  print(\"------------------------------------------------------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1lQGDaTLn50"
      },
      "source": [
        "## might have to del this cell\n",
        "\n",
        "x_min, x_max = X_train.loc[:, 'LAT'].min() - 1, X_train.loc[:, 'LAT'].max() + 1\n",
        "y_min, y_max = X_train.loc[:, 'YEAR'].min() - 1, X_train.loc[:, 'YEAR'].max() + 1\n",
        "print(x_min, x_max, y_min, y_max)\n",
        "# Just checking YEAR column as it has come out as the most important feature which isn't appropriate I think\n",
        "\n",
        "plt.hist(eq_data['YEAR'], 100, range=(1800, 2100), density=True, facecolor='b', alpha=0.75)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udWt4B46ZUdK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}